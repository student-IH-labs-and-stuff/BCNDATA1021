{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-principle",
   "metadata": {},
   "source": [
    "Always good practice in the beginning: Where am I, what's in my folder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without the ! also works in most cases\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-messaging",
   "metadata": {},
   "source": [
    "# Load all three files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('lab-customer-analysis-round-1/files_for_lab/csv_files/file1.csv')\n",
    "df2 = pd.read_csv('lab-customer-analysis-round-1/files_for_lab/csv_files/file2.csv')\n",
    "df3 = pd.read_csv('lab-customer-analysis-round-1/files_for_lab/csv_files/file3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-parent",
   "metadata": {},
   "source": [
    "# Show all shapes of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "[df.shape for df in (df1, df2, df3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or one by one:\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-lesson",
   "metadata": {},
   "source": [
    "# Standardize header names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-coalition",
   "metadata": {},
   "source": [
    "First check, what's wrong with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-mainland",
   "metadata": {},
   "source": [
    "column names are not consistently in lower or upper case and also seperated by whitespaces. We need to get rid of whitespaces and make them all lower case.\n",
    "\n",
    "Problem: We need to have the **same** job done for **several** things.\n",
    "Solution:\n",
    "\n",
    "    * Step 1: We get a solution for 1 case (1 column name)\n",
    "    * Step 2: We scale up and apply that solution to all column names\n",
    "    \n",
    "Good problem solving approach in data analytics in general! Keep it in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-audio",
   "metadata": {},
   "source": [
    "Let's make it work for one column. We write a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_col(col):\n",
    "    return col.lower().replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-escape",
   "metadata": {},
   "source": [
    "Let's try it for the string `Total Claim Amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_col(\"Total Claim Amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-organizer",
   "metadata": {},
   "source": [
    "works fine, so lets apply it to all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df1.columns.to_list():\n",
    "    print(standardize_col(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a list were we store all these standardized\n",
    "# columns in\n",
    "stdzd_cols = []\n",
    "for col in df1.columns.to_list():\n",
    "    stdzd_cols.append(standardize_col(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-indonesian",
   "metadata": {},
   "source": [
    "and then overwrite these columns with the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = stdzd_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether change was effective\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-insured",
   "metadata": {},
   "source": [
    "now, we need to perfom this job on two other dataframes. So why not write a function that receives a dataframe and standardizes the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_df_colnames(df):\n",
    "    #this is just copy pasted from above\n",
    "    stdzd_cols = []\n",
    "    for col in df.columns.to_list():\n",
    "        # making use of the standardize_col function from above\n",
    "        stdzd_cols.append(standardize_col(col))\n",
    "    # in the end, overwrite columns with standardized ones\n",
    "    df.columns = stdzd_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize df2\n",
    "standardize_df_colnames(df2)\n",
    "# standardize df3\n",
    "standardize_df_colnames(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06080e35",
   "metadata": {},
   "source": [
    "#### column naming mismatch \n",
    "\n",
    "notice that there is a different naming convention for one of the data sources, state <> st \n",
    "\n",
    "+ i recommend that you sort out the naming of the column in the original data frame, to make life easier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.rename(columns={'state':'st'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-north",
   "metadata": {},
   "source": [
    "### Why do we standardize?\n",
    "\n",
    "* it's more convenient working with a dataframe for which you don't have to remember exactly which column name is capital / lower case letters\n",
    "* sometimes you see that columns are not refered to as `df3['vehicle_class']` but `df3.vehicle_class`. The latter would not be possible if you dont have the `_` between the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-cathedral",
   "metadata": {},
   "source": [
    "# Concatenate dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-universe",
   "metadata": {},
   "source": [
    "How should we concatenate the dataframes? Stick them together horizontally (column wise) or vertically (row wise). Let's look at them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-forum",
   "metadata": {},
   "source": [
    "column wise it is! We don't need to think about ordering the column names of the individual data frames. Pandas takes care of that when concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we concatenate rowwise. We don't need to change \"axis\" parameter,\n",
    "# because axis=0 (rowwise) is the default\n",
    "df_all = pd.concat([df1, df2, df3],\n",
    "                  # axis=0,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-treasure",
   "metadata": {},
   "source": [
    "Now, we need to reset the index, because the index numbers are still from the original dataframe (check e.g. that we have 12074 rows, but our index ends at 7069)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)\n",
    "# or do (same effect) df_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-chess",
   "metadata": {},
   "source": [
    "Let's insert a data cleaning step here already and delete all the rows that contain **only** `NaN` values. Because what's the point in having them? :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates you a boolean mask for every row that has NaN values only! check also .any() etc...\n",
    "df_all.isna().all(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-tongue",
   "metadata": {},
   "source": [
    "filter the rows by applying this \"sieve\" to the dataframe. We're using `~` here because we want to negate / invert the boolean \"sieve\" and **keep** the rows where it says `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[~df_all.isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-nancy",
   "metadata": {},
   "source": [
    "## drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-holiday",
   "metadata": {},
   "source": [
    "## filter negative incomes\n",
    "\n",
    "because we regard those as data errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['income'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-glenn",
   "metadata": {},
   "source": [
    "# Which columns are numerical?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-russian",
   "metadata": {},
   "source": [
    "See [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.select_dtypes(np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-difficulty",
   "metadata": {},
   "source": [
    "or even more convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nums = df_all._get_numeric_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-private",
   "metadata": {},
   "source": [
    "# Which are categorical?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-support",
   "metadata": {},
   "source": [
    "Short answer? The rest :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all numerical columns\n",
    "df_cats = df_all.drop(columns=df_nums.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-europe",
   "metadata": {},
   "source": [
    "# Understand the meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-enlargement",
   "metadata": {},
   "source": [
    "Usually: Check the documentation of the study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-dealing",
   "metadata": {},
   "source": [
    "# Perform the data cleaning operations mentioned so far in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-material",
   "metadata": {},
   "source": [
    "Data we have\n",
    "* `df_nums`\n",
    "* `df_cats`\n",
    "\n",
    "Let's look what's dirty here. Caution: We don't want to drop rows after we split into dataframes of categorical and numerical columns because later, after data cleaning, when we stitch them back together, we might end up with mismatching row lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for number of null values in every column\n",
    "df_cats.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-albania",
   "metadata": {},
   "source": [
    "or with percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats.isna().sum()/len(df_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-escape",
   "metadata": {},
   "source": [
    "same with numerics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nums.isna().sum()/len(df_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-export",
   "metadata": {},
   "source": [
    "Numerics are clean of null values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-equilibrium",
   "metadata": {},
   "source": [
    "### 'st'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['st'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-seafood",
   "metadata": {},
   "source": [
    "The states column `st` has a lot of null values (77%) we need to keep that in mind. We have the feeling though that it might turn out as an important column, since the origin of our customers might be good predictors (we'll learn about that later in the week) for other things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-distributor",
   "metadata": {},
   "source": [
    "### 'gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-stake",
   "metadata": {},
   "source": [
    "* let's clean all the entries that are not `F` or `M` and turn it into either `F` or `M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a task like this can only be solved manually\n",
    "# a good approach here is a dictionary, because, what do we want to do? Translating!\n",
    "trans_dct = {'Male': 'M',\n",
    "             'female':'F',\n",
    "             'Femal':'F',\n",
    "             'F':'F',\n",
    "             'M':'M',\n",
    "            }\n",
    "\n",
    "# now, we can write ourselve a function, that does the translate\n",
    "# and use df_cats['gender'].apply(my_function)\n",
    "# but we can also make use of the map function, which is way faster (although speed is not \n",
    "# important at this point yet)!\n",
    "df_cats['gender'] = df_cats['gender'].map(trans_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-article",
   "metadata": {},
   "source": [
    "BAM! Clean!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-mitchell",
   "metadata": {},
   "source": [
    "### 'education'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-organ",
   "metadata": {},
   "source": [
    "### 'customer_lifetime_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['customer_lifetime_value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-sender",
   "metadata": {},
   "source": [
    "* they're all meant as percent\n",
    "* but some come with % at the end\n",
    "* we're casting the values as str first\n",
    "* then replace all %'s with nothing\n",
    "* turn the result into float\n",
    "* and finally multiply by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_clv(x):\n",
    "    return float(str(x).replace('%', ''))*100\n",
    "df_cats['customer_lifetime_value'] = df_cats['customer_lifetime_value'].apply(clean_clv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-forge",
   "metadata": {},
   "source": [
    "### 'number of open complaints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['number_of_open_complaints'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-money",
   "metadata": {},
   "source": [
    "We see that we have either values of 0, 1, 2, 3, 4 or 5. But then also `1/0/00`, `1/1/00`, `1/2/00`, `1/3/00`, `1/4/00` and `1/5/00`, which contain only redundant info except the middle number. So we create ourselves again a trans dict and roll like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_dct = {0:0,\n",
    "             1:1,\n",
    "             2:2,\n",
    "             3:3,\n",
    "             4:4,\n",
    "             5:5,\n",
    "             '1/0/00' :0, \n",
    "             '1/1/00' :1, \n",
    "             '1/2/00' :2, \n",
    "             '1/3/00' :3, \n",
    "             '1/4/00' :4, \n",
    "             '1/5/00' :5, \n",
    "            }\n",
    "\n",
    "df_cats['number_of_open_complaints'] = df_cats['number_of_open_complaints'].map(trans_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats['number_of_open_complaints'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-petite",
   "metadata": {},
   "source": [
    "Clean!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
